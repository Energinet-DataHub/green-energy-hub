name: Databricks Integration Test

on:
  #push:
  #  pull_request:
  #    branches: 
  #      - main 
  workflow_dispatch:
    inputs:
      resource_group_name:
        description: 'Resource Group Name You Want to Use for Deployment'
        required: true
        default: ''
      env_name:
        description: 'Env name used to postfix resources names'
        required: true
        default: ''
      client_id:
        description: 'Service Principal Client (Application) Id'
        required: true
        default: ''
      object_id:
        description: 'Service Principal Object Id'
        required: true
        default: ''
      client_secret:
        description: 'Service Principal Secret'
        required: true
        default: ''

jobs:
  build-and-deploy:
    name: Run DataBricks integration test
    runs-on: ubuntu-latest
    env:
      WHEEL_VERSION: "1.4"
      WHEEL_STORAGE_ADDRESS: https://enrgtwheels.blob.core.windows.net/wheels/
      MAIN_PYTHON_FILE: "dbfs:/streaming/enrichment_and_validation.py"

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Construct Wheel File Name
      uses: ./.github/actions/construct-wheel-file-name

    - name: Read Pipeline Configuration
      uses: ./.github/actions/read-pipeline-configuration
      with:
        config-file: 'integration-tests.json'

    - name: Set Environment Secrets
      run: |  
        echo "ARM_TENANT_ID=${{ secrets.TENANT_ID }}" >> $GITHUB_ENV
        echo "ARM_CLIENT_ID=${{ secrets[env.GITHUB_SECRET_NAME_SPN_ID] }}" >> $GITHUB_ENV
        echo "ARM_CLIENT_OBJECT_ID=${{ secrets[env.GITHUB_SECRET_NAME_SPN_OBJECT_ID] }}" >> $GITHUB_ENV
        echo "ARM_CLIENT_SECRET=${{ secrets[env.GITHUB_SECRET_NAME_SPN_SECRET] }}" >> $GITHUB_ENV
        echo "ARM_SUBSCRIPTION_ID=${{ secrets[env.GITHUB_SECRET_NAME_SUBSCRIPTION_ID] }}" >> $GITHUB_ENV

    # Run only if workflow started on demand (manually triggered)
    - name: Set Variables from Pipeline Inputs
      uses: ./.github/actions/override_env_settings
      if:   github.event.inputs.resource_group_name

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1.2.1
      with:
        terraform_wrapper: false

    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.7' # Version range or exact version of a Python version to use, using SemVer's version range syntax
        architecture: 'x64' # optional x64 or x86. Defaults to x64 if not specified

    - name: Azure CLI Install and Login
      uses: ./.github/actions/azure-cli-install-login

    - name: Setup storage credentials
      run: |
        booking_connection_string=$(az storage account show-connection-string --name "enrgstate${{ env.ENV_NAME }}" --resource-group ${{ env.RESOURCE_GROUP_NAME }} --output tsv)
        echo ::add-mask::$booking_connection_string
        echo "BOOKING_STORAGE_CONNECTION_STRING=$booking_connection_string" >> $GITHUB_ENV
        output_storage_key=$(az storage account keys list --resource-group ${{ env.RESOURCE_GROUP_NAME }} --account-name "timeseriesdata${{ env.ENV_NAME }}" --query '[0].value' -o tsv)
        echo ::add-mask::$output_storage_key
        echo "OUTPUT_STORAGE_KEY"=$output_storage_key >> $GITHUB_ENV

    # book resources
    - name: Book resources for integration test
      id: rb-book
      uses: ./.github/actions/resource-book
      with:
        action: book
        azure-storage-connection-string: ${{ env.BOOKING_STORAGE_CONNECTION_STRING }}
        group: 'INPUT,COSMOS,OUTPUT_CONTAINER'
        state-blob: resource-booking-state
        state-container: it-booking-state

    - name: Obtain Databricks Workspace ID and Url
      id: obtain-db-id-url
      uses: ./.github/actions/obtain-databricks-id-url

    - name: Obtain Keyvault ID and Name
      id: obtain-keyvault-id-name 
      uses: ./.github/actions/obtain-keyvault-id-name

      # Try not to reference TF_VAR variables in pipeline yml files, only values should be set and they should be read in terraform only
      # rather create duplicate ENV pipeline vatiable if needed to separate concerns
    - name: Set TF Vars
      run: |
        receiver_eh_input_secret_name="receiver-${{ env.RB_OUT_INPUT }}"
        streaming_container_name=${{ env.RB_OUT_OUTPUT_CONTAINER }}
        cosmos_coll=${{ env.RB_OUT_COSMOS }}

        echo "TF_VAR_receiver_eh_input_secret_name=$receiver_eh_input_secret_name" >> $GITHUB_ENV
        echo "TF_VAR_streaming_container_name=$streaming_container_name" >> $GITHUB_ENV
        echo "TF_VAR_cosmos_coll=$cosmos_coll" >> $GITHUB_ENV
        echo "TF_VAR_environment=${{ env.ENV_NAME }}" >> $GITHUB_ENV 
        echo "TF_VAR_resource_group_name=${{ env.RESOURCE_GROUP_NAME }}" >> $GITHUB_ENV
        echo "TF_VAR_environment=${{ env.ENV_NAME }}" >> $GITHUB_ENV
        echo "TF_VAR_keyvault_id=${{ steps.obtain-keyvault-id-name.outputs.keyvault-id }}" >> $GITHUB_ENV
        echo "TF_VAR_databricks_id=${{ steps.obtain-db-id-url.outputs.workspace-id }}" >> $GITHUB_ENV 
        echo "TF_VAR_python_main_file=${{ env.MAIN_PYTHON_FILE}}" >> $GITHUB_ENV 
      
    - name: Databricks CLI Install And Connect
      uses: ./.github/actions/databricks-cli-install-connect
      with:
        workspace-url: ${{ steps.obtain-db-id-url.outputs.workspace-url }}

    - uses: suisei-cn/actions-download-file@v1
      name: Download the Wheel File
      with:
        url: "${{ env.WHEEL_STORAGE_ADDRESS }}${{ env.WHEEL_FILE_NAME }}"
        target: wheels/

    - name: Copy Wheel File to Databricks Workspace
      uses: ./.github/actions/copy-wheel-file-to-databricks

    - name: Copy Job Definition to DBFS
      id: copy_files_todbfs
      run: |    
        dbfs cp --overwrite ./src/streaming/enrichment_and_validation.py ${{ env.MAIN_PYTHON_FILE }}

    - uses: suisei-cn/actions-download-file@v1
      name: Download Custom CosmosDB connector
      with:
        # Temporarily used package. See comments in .devcontainer/spark-defaults.conf.
        url: "https://github.com/FabianMeiswinkel/PrivateNugetPackages/raw/master/azure-cosmosdb-spark_3.0.0_2.12-3.6.2-uber.jar"
        target: cosmosdb_connector/

    - name: Copy Custom CosmosDB connector to DBFS
      id: copy_cosmosdb_connector_todbfs
      run: |    
        dbfs cp --overwrite ./cosmosdb_connector/azure-cosmosdb-spark_3.0.0_2.12-3.6.2-uber.jar dbfs:/streaming/cosmosdb-connector.jar

    - name: Terraform Databricks Init
      working-directory: ./build/terraform/databricks_streaming_job_it
      run: terraform init

    # Resource in command must match with resource name in main.tf
    # after feature: https://github.com/databrickslabs/terraform-provider-databricks/issues/389 
    # is available this should be changed and job should not be recreated on each run - taint to be removed           
    - name: Terraform Databricks Apply
      id: terraform-apply
      working-directory: ./build/terraform/databricks_streaming_job_it
      run: |
        terraform apply -no-color -auto-approve
        terraform taint "module.streaming_job.databricks_job.streaming_job"
        echo "::set-output name=job-id::$(terraform output databricks_job_id)"
      continue-on-error: false

    - name: Databricks CLI Run the Job
      id: run-job
      uses: ./.github/actions/databricks-cli-run-jobs
      with:
        job-ids: ${{ steps.terraform-apply.outputs.job-id }}

    - name: Check Job Status
      working-directory: ./build
      run: |
        pip install configargparse
        pip install requests
        python -u job_status_check.py --job-run-ids  ${{ steps.run-job.outputs.job-run-ids }} --retries 2 --databricks-url 'https://${{ steps.obtain-db-id-url.outputs.workspace-url }}' --token ${{ env.DATABRICKS_AAD_TOKEN }}
   
    - name: Setup Sender Event Hub Connection String
      run: |
        keyvault_secret_details=$(az keyvault secret show --name "sender-${{ env.RB_OUT_INPUT }}" --vault-name ${{ steps.obtain-keyvault-id-name.outputs.keyvault-name }})
        sender_eh_connection_string=$(echo $keyvault_secret_details | python -c "import sys, json; print(json.load(sys.stdin)['value'])")
        echo ::add-mask::$sender_eh_connection_string
        echo "SENDER_EVENT_HUB_CONNECTION_STRING=$sender_eh_connection_string" >> $GITHUB_ENV

    - name: Integration Tests
      uses: ./.github/actions/databricks-integration-test
      with:
        storage-account-name: "timeseriesdata${{ env.ENV_NAME }}"
        storage-account-key: ${{ env.OUTPUT_STORAGE_KEY }}
        storage-container-name: ${{ env.RB_OUT_OUTPUT_CONTAINER }}
        input-eh-connection-string: ${{ env.SENDER_EVENT_HUB_CONNECTION_STRING }} 
        delta-lake-output-path: delta/meter-data

    #Final steps, running always, event if workflow has been canceled
    - name: Terraform Destroy
      id: terraform-destroy
      if: ${{ always() && steps.terraform-apply.outputs.job-id != '' }}
      working-directory: ./build/terraform/databricks_streaming_job_it
      run: |
        terraform destroy -no-color -auto-approve

    - name: Release Resources After Integration Test
      id: rb-release
      if: ${{ always() && steps.rb-book.outputs.result == 'True' }}
      uses: ./.github/actions/resource-book
      with:
        action: release
        azure-storage-connection-string: ${{ env.BOOKING_STORAGE_CONNECTION_STRING }}
        resource: ${{ steps.rb-book.outputs.output }}
        state-blob: resource-booking-state
        state-container: it-booking-state
